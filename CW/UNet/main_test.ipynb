{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "950440b6-992d-4c0b-827b-d728bcbf2094",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os.path\n",
    "import argparse\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from UNet_model import *\n",
    "from dataset_handler import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9c38fd2-f55d-4c93-9b4a-0ed4f939bea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset CityscapesDataset\n",
      "    Number of images: 500\n",
      "    Split: val\n",
      "    Mode: gtFine\n",
      "    Root Location: /mnt/data/course/psarin/inm705/\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/adcy347/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    }
   ],
   "source": [
    "# cityscapes dataset loading\n",
    "img_data = CityscapesDataset(\"/mnt/data/course/psarin/inm705/\", split='val', mode='fine')\n",
    "img_batch = torch.utils.data.DataLoader(img_data, batch_size=64, shuffle=True, num_workers=4)\n",
    "print(img_data)\n",
    "#/mnt/data/course/psarin/inm705/leftImg8bit\n",
    "#/mnt/data/course/psarin/inm705/gtFine_trainvaltest/gtFine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28908ea1-1460-4d5a-9b09-90c6965968d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating unet model...\n"
     ]
    }
   ],
   "source": [
    "# initiate generator\n",
    "print(\"creating unet model...\")\n",
    "generator = nn.DataParallel(UnetGenerator(3, img_data.num_classes, 64), device_ids=[i for i in range(1)]).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4900c6fe-d52a-45e6-b6e6-873dd4e192b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    - model restored from file....\n",
      "    - filename = ./checkpoints_epochs/train_UNet.pkl\n"
     ]
    }
   ],
   "source": [
    "# load pretrained model if it is there\n",
    "file_model = './checkpoints_epochs/train_UNet.pkl'\n",
    "if os.path.isfile(file_model):\n",
    "    generator = torch.load(file_model)\n",
    "    print(\"    - model restored from file....\")\n",
    "    print(\"    - filename = %s\" % file_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59e51db2-fe4b-4ca2-ab2f-8d9b726704a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "evaluating network (will take a while)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/adcy347/.local/lib/python3.9/site-packages/torchvision/transforms/functional.py:442: UserWarning: Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. Please use InterpolationMode enum.\n",
      "  warnings.warn(\n",
      "/users/adcy347/.local/lib/python3.9/site-packages/torchvision/transforms/functional.py:442: UserWarning: Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. Please use InterpolationMode enum.\n",
      "  warnings.warn(\n",
      "/users/adcy347/.local/lib/python3.9/site-packages/torchvision/transforms/functional.py:442: UserWarning: Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. Please use InterpolationMode enum.\n",
      "  warnings.warn(\n",
      "/users/adcy347/.local/lib/python3.9/site-packages/torchvision/transforms/functional.py:442: UserWarning: Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. Please use InterpolationMode enum.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NETWORK RESULTS\n",
      "    - avg timing = 0.7080 (sec)\n",
      "    - avg accuracy = 0.8508\n"
     ]
    }
   ],
   "source": [
    "# Loop through the dataset and evaluate how well the network predicts\n",
    "print(\"\\nevaluating network (will take a while)...\")\n",
    "history_accuracy = []\n",
    "history_time = []\n",
    "for idx_batch, (imagergb, label_class, labelrgb) in enumerate(img_batch):\n",
    "\n",
    "    # send to the GPU and do a forward pass\n",
    "    start_time = time.time()\n",
    "    x = Variable(imagergb).cuda(0)\n",
    "    y_ = Variable(label_class).cuda(0)\n",
    "    y = generator.forward(x)\n",
    "    end_time = time.time()\n",
    "\n",
    "    # we \"squeeze\" the groundtruth if we are using cross-entropy loss\n",
    "    # this is because it expects to have a [N, W, H] image where the values\n",
    "    # in the 2D image correspond to the class that that pixel should be 0 < pix[u,v] < classes\n",
    "    \n",
    "    y_ = torch.squeeze(y_)\n",
    "\n",
    "    # max over the classes should be the prediction\n",
    "    # our prediction is [N, classes, W, H]\n",
    "    # so we max over the second dimension and take the max response\n",
    "    # if we are doing rgb reconstruction, then just directly save it to file\n",
    "    pred_class = torch.zeros((y.size()[0], y.size()[2], y.size()[3]))\n",
    "\n",
    "    for idx in range(0, y.size()[0]):\n",
    "        pred_class[idx] = torch.argmax(y[idx], dim=0).cpu().int()\n",
    "        #pred_rgb[idx] = img_data.class_to_rgb(maxindex)\n",
    "\n",
    "\n",
    "    # unsqueese so we have [N, 1, W, H] size\n",
    "    # this allows for debug saving of the images to file...\n",
    "    pred_class = pred_class.unsqueeze(1).float()\n",
    "    label_class = label_class.unsqueeze(1).float()\n",
    "\n",
    "    # now compare the groundtruth to the predicted\n",
    "    # we should record the accuracy for the class\n",
    "    acc_sum = (pred_class == label_class).sum()\n",
    "    acc = float(acc_sum) / (label_class.size()[0]*label_class.size()[2]*label_class.size()[3])\n",
    "    history_accuracy.append(acc)\n",
    "    history_time.append((end_time-start_time))\n",
    "\n",
    "    # debug saving generated classes to file\n",
    "    #v_utils.save_image(pred_class.float()/img_data.num_classes, \"./result/gen_image_{}_{}.png\".format(0, idx_batch))\n",
    "    #v_utils.save_image(label_class.float()/img_data.num_classes, \"./result/label_image_{}_{}.png\".format(0, idx_batch))\n",
    "    #v_utils.save_image(x.cpu().data, \"./result/original_image_{}_{}.png\".format(0, idx_batch))\n",
    "\n",
    "\n",
    "# finally output the accuracy\n",
    "print(\"\\nNETWORK RESULTS\")\n",
    "print(\"    - avg timing = %.4f (sec)\" % (sum(history_time)/len(history_time)))\n",
    "print(\"    - avg accuracy = %.4f\" % (sum(history_accuracy)/len(history_accuracy)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac17c26-059c-4e64-ac76-e315f450819e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
